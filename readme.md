# Awesome_Multimodal_Learning

<div align='center'>
  <img src=https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg >
  <img src=https://img.shields.io/github/forks/wangskyone/Awesome_multimodal.svg?style=social >
  <img src=https://img.shields.io/github/stars/wangskyone/Awesome_multimodal.svg?style=social >
  <img src=https://img.shields.io/github/watchers/wangskyone/Awesome_multimodal.svg?style=social >
 </div>   

## ğŸ“’Motivation
- Tracking State-of-the-art multi-modal fusion and alignment methods and Vision-Language Models
- Analyzing the State-of-the-art papers from top conferences, including but not limited to ICCV, CVPR, ICML, ICLR, and NIPS.
- Replicating classical paper results with SOTA performance.

## Survey and Papers 
- [Multimodal Learning with Transformers: A Survey](https://arxiv.org/abs/2206.06488) â­ï¸â­ï¸â­

## Experiments Visualization
