# Awesome_Multimodal_Learning

<div align='center'>
  <img src=https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg >
  <img src=https://img.shields.io/github/forks/wangskyone/Awesome_multimodal.svg?style=social >
  <img src=https://img.shields.io/github/stars/wangskyone/Awesome_multimodal.svg?style=social >
  <img src=https://img.shields.io/github/watchers/wangskyone/Awesome_multimodal.svg?style=social >
 </div>   

## üìíMotivation
- Tracking State-of-the-art multi-modal fusion and alignment methods and Vision-Language Models
- Analyzing the State-of-the-art papers from top conferences, including but not limited to ICCV, CVPR, ICML, ICLR, and NIPS.
- Replicating classical paper results with SOTA performance.

## Survey
- [Multimodal Learning with Transformers: A Survey](https://arxiv.org/abs/2206.06488) ‚≠êÔ∏è‚≠êÔ∏è‚≠ê


## Multimodal Fusion
- [Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning (ICLR24)](https://arxiv.org/abs/2206.06488), [code](https://github.com/joshuaxiao98/ITHP)


## classic Papers
- [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490)


## Experiments Visualization
